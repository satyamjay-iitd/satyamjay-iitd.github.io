{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Curriculum Vitae","text":""},{"location":"#curriculum-vitae","title":"Curriculum Vitae","text":"Github Contributions"},{"location":"contact/","title":"Contact","text":"<ul> <li> <p>Email: </p> <p>satyamj@sit.iitd.ac.in satyamjay030@gmail.com </p> </li> <li> <p>Location: </p> <p>Cloud Lab (411), SIT, IIT Delhi  </p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/2025/02/23/notes-on-checkpointingrecovery-in-message-passing-systems/","title":"Notes on checkpointing/recovery in message passing systems","text":""},{"location":"blog/2025/02/23/notes-on-checkpointingrecovery-in-message-passing-systems/#notes-on-checkpointingrecovery-in-message-passing-systems","title":"Notes on checkpointing/recovery in message passing systems","text":""},{"location":"blog/2025/02/23/notes-on-checkpointingrecovery-in-message-passing-systems/#terminologies","title":"Terminologies","text":"<ol> <li>Process:-</li> <li>Message:-</li> <li>(Non)Volatile Storage:-</li> </ol> <p>In order to make systems fault tolerant, checkpointing is employed. In this method different processes periodically checkpoint their state into a non-volatile storage. In the presence of a failure the checkpointed state is use to recover the processes to a earlier consistent state.</p> <p>Things to optimize:-</p> <ol> <li>(Time spent in checkpointing) / (Time spent in processing).</li> <li>Size of the checkpointed state.</li> <li>Minimize the progress loss during recovery.</li> <li>Minimize recovery time.</li> </ol> <p>Checkpointing requires</p>"},{"location":"blog/2024/12/21/my-first-rust-macro/","title":"My first Rust macro.","text":"","tags":["rust"]},{"location":"blog/2024/12/21/my-first-rust-macro/#my-first-rust-macro","title":"My first Rust <code>macro</code>.","text":"","tags":["rust"]},{"location":"blog/2024/12/21/my-first-rust-macro/#background","title":"Background","text":"<p>I am new to Rust programming, and currently working on my first large rust project. As part of my research we are building a data streaming framework(will be released as OSS after the paper is published). In this system there are different types of messages that can be passed between different componenets of the system.</p> <p>For example:- </p>Message<pre><code>pub enum FromWorkerToCoord {\n    Done(String),\n    ReconfAck((String, Box&lt;ReconfMsg&gt;)),\n    PollTPResponse((String, RecvThrougputMsg)),\n    Handshake(String),\n}\n</code></pre> <p>For each such message we had to write a <code>Codec</code>. A codec can <code>encode</code> an object to bytes that can be sent over wire, as well as <code>decode</code> bytes recieved over the wire to an object. We are using bincode for this purpose.</p> <p>A Codec implementation for the above struct looks something like this:-</p> Codec<pre><code>#[derive(Clone)]\npub struct FromWorkerToCoordCodec {\n    config: bincode::config::Configuration,\n    length_codec: tokio_util::codec::LengthDelimitedCodec,\n}\n\nimpl FromWorkerToCoordCodec {\n    pub fn new() -&gt; Self {\n        FromWorkerToCoordCodec {\n            config: bincode::config::standard(),\n            length_codec: tokio_util::codec::LengthDelimitedCodec::builder()\n                .length_field_length(4)\n                .max_frame_length(u32::MAX as usize)\n                .new_codec(),\n        }\n    }\n}\n\nimpl tokio_util::codec::Encoder&lt;FromWorkerToCoord&gt; for FromWorkerToCoordCodec {\n    type Error = std::io::Error;\n\n    fn encode(\n        &amp;mut self,\n        item: #msg_name,\n        dst: &amp;mut bytes::BytesMut,\n    ) -&gt; Result&lt;(), Self::Error&gt; {\n        ...\n        }\n}\nimpl tokio_util::codec::Decoder for FromWorkerToCoordCodec {\n    type Item = FromWorkerToCoord;\n    type Error = std::io::Error;\n\n    fn decode(&amp;mut self, src: &amp;mut BytesMut) -&gt; Result&lt;Option&lt;FromWorkerToCoord&gt;, Self::Error&gt; {\n      ...\n    }\n}\n</code></pre>","tags":["rust"]},{"location":"blog/2024/12/21/my-first-rust-macro/#motivation","title":"Motivation","text":"<p>We had bunch of such messages and needed a similar Codec for all of them. And there is no difference in logic for different Codecs, they all use bincode internally, and the only difference is the Message that they are encoding/decoding. Hence there is lot of code duplication. I had heared about <code>macros</code> in rust that allows us to prevent this.</p>","tags":["rust"]},{"location":"blog/2024/12/21/my-first-rust-macro/#macros-primer","title":"Macros primer","text":"<p>A macro is a metaprogramming tool, that allows us to write code that generates code. The macro expansion happens at compile time so there is no runtime overhead.</p> <p>A very good introduction to Rust macro can be found here. The summary of the above blog is as follows:-  There are two types of macros:-</p> <ol> <li>Declarative:- Replaces the macro invocation by the code generated by the given marco. It internally uses pattern matching based logic to generate the code.</li> <li>Procedural:- Takes in an AST and generates a new AST. Since it has access to the entire AST it can use arbitrary logic to generate code.</li> </ol> Type Usecase Examples declarative Pattern matching for repetitve code <code>vec!</code>, <code>println!</code> procedural-attribute Modify functions or structs using attributes <code>#[tokio::main]</code> procedural-derive Automatic implementation of traits <code>#[derive(Clone)]</code> procedural-function Generate new functions using some parameters","tags":["rust"]},{"location":"blog/2024/12/21/my-first-rust-macro/#implementation","title":"Implementation","text":"<p>Since I had to implement <code>Encoder</code> and <code>Decoder</code> trait I went with <code>derive_macro</code>.</p> <ol> <li>Created a new crate using <code>cargo new --lib codec-derive</code></li> <li>Instructed the compiler that this library implements a procedural macro by adding the following lines in <code>Cargo.toml</code> <pre><code>[lib]\nproc-macro = true\nCreated a new crate using `cargo new --lib codec-derive`\n</code></pre> Now lets see the code. Click on the + symbol to read the explanation.</li> </ol> Entrypoint<pre><code>use proc_macro::{self, TokenStream};\nuse syn::parse_macro_input; // (1)\nuse quote::{format_ident, quote};  // (2)\n\n#[proc_macro_derive(Codec)]\npub fn codec_derive(input: TokenStream) -&gt; TokenStream {\n    let input = parse_macro_input!(input as syn::DeriveInput);  // (3)\n\n    let name: &amp;syn::Ident = &amp;input.ident;\n    let codec_name: syn::Ident = format_ident!(\"{}Codec\", name);  // (4)\n\n    let generics = &amp;input.generics;\n    // (5)\n    if generics.params.is_empty() {\n        gen_without_generic(name, codec_name)\n    } else {\n        gen_with_generic(name, codec_name)\n    }\n}\n</code></pre> <ol> <li>syn parses the TokenStream into an AST.</li> <li>quote converts AST into a TokenStream.</li> <li>Using syn library first we convert the input token stream into an AST.</li> <li>We then generate the name of the output struct as <code>{InputStructName}Codec</code>. So the name of the generated Codec struct for <code>FromWorkerToCoord</code> will be <code>FromWorkerToCoordCodec</code>.</li> <li>Codegen logic.</li> </ol> <p>Some of our messages were generic, for example:- </p>Generic Message<pre><code>pub enum FromPeerToPeer&lt;S&gt; {\n    Handshake(String),\n    State((StateBatch&lt;S&gt;, ReconfId)),\n}\n</code></pre> <p>To deal with this I had to create seperate logic for messages with a generic parameter and without generic  parameter. There has to be a better way to do this, I will refactor this when I learn that.</p> <p>Now lets see the main code generation code. </p>Codegen without generics<pre><code>fn gen_without_generic(msg_name: &amp;syn::Ident, codec_name: syn::Ident) -&gt; TokenStream {\n    #[derive(Clone)]\n    pub struct #codec_name {   // (2)\n        config: bincode::config::Configuration,\n        length_codec: tokio_util::codec::LengthDelimitedCodec,\n    }\n\n    impl #codec_name {\n        pub fn new() -&gt; Self {\n            #codec_name {\n                config: bincode::config::standard(),\n                length_codec: tokio_util::codec::LengthDelimitedCodec::builder()\n                    .length_field_length(4)\n                    .max_frame_length(u32::MAX as usize)\n                    .new_codec(),\n            }\n        }\n    }\n\n    impl tokio_util::codec::Encoder&lt;#msg_name&gt; for #codec_name {\n        type Error = std::io::Error;\n\n        fn encode(\n            &amp;mut self,\n            item: #msg_name,    // (3)\n            dst: &amp;mut bytes::BytesMut,\n        ) -&gt; Result&lt;(), Self::Error&gt; {\n            let encoded_data = bincode::encode_to_vec(&amp;item, self.config)\n                .map_err(|_| std::io::Error::new(\n                    std::io::ErrorKind::InvalidData,\n                    \"Failed to encode data\",\n                ))?;\n\n            self.length_codec\n                .encode(bytes::Bytes::from(encoded_data), dst)\n                .map_err(|_| std::io::Error::new(\n                    std::io::ErrorKind::InvalidData,\n                    \"Couldn't encode length-delimited data\",\n                ))?;\n\n                Ok(())\n            }\n    }\n    impl tokio_util::codec::Decoder for #codec_name {\n        type Item = #msg_name;\n        type Error = std::io::Error;\n\n        fn decode(&amp;mut self, src: &amp;mut BytesMut) -&gt; Result&lt;Option&lt;#msg_name&gt;, Self::Error&gt; {\n            let frame = match self.length_codec.decode(src).map_err(|_| {\n                std::io::Error::new(\n                    ErrorKind::InvalidData,\n                    \"Couldn't decode length-delimited data\",\n                )\n            })? {\n                Some(frame) =&gt; frame,\n                None =&gt; return Ok(None), // Not enough data yet\n            };\n\n            let (message, _) = bincode::decode_from_slice(&amp;frame, self.config).map_err(|_| {\n                std::io::Error::new(\n                    ErrorKind::InvalidData,\n                    \"Couldn't decode message from bitcode\",\n                )\n            })?;\n\n            Ok(Some(message))\n        }\n    }\n}\n</code></pre> <ol> <li> <p>Remember that we need to return the code as a TokenStream. <code>quote</code> provides a very handy macro that takes in a rust looking template and produces a valid Rust TokenStream.</p> </li> <li> <p><code>quote</code> will replace this with the codec name.</p> </li> <li> <p><code>quote</code> will replace this with the message name.</p> </li> </ol> <p>Thats it!!!.</p>","tags":["rust"]},{"location":"blog/2024/12/21/my-first-rust-macro/#usage","title":"Usage","text":"<p>Now that we have implemented our derive_macro we can use it as follows:- </p><pre><code>#[derive(Debug, PartialEq, Eq, Clone, Encode, Decode, Codec)]\npub enum FromCoordToWorker {\n    Done,\n    PollTPRequest(u64),\n    Reconf(Box&lt;ReconfMsg&gt;),\n    Exit,\n}\n</code></pre> Now when we create a new message we just need to add <code>#[derive(Codec)]</code> and we will have a codec generated for that message. Note that it is required that message also implements <code>Encode</code> and <code>Decode</code> in order to derive a Codec.","tags":["rust"]},{"location":"blog/2025/03/26/why-struct-stditermap-doesnt-constraint-i-to-be-an-iterator/","title":"Why struct std::iter::Map doesn't constraint 'I' to be an Iterator?","text":"","tags":["rust"]},{"location":"blog/2025/03/26/why-struct-stditermap-doesnt-constraint-i-to-be-an-iterator/#why-struct-stditermap-doesnt-constraint-i-to-be-an-iterator","title":"Why struct std::iter::Map doesn't constraint 'I' to be an Iterator?","text":"","tags":["rust"]},{"location":"blog/2025/03/26/why-struct-stditermap-doesnt-constraint-i-to-be-an-iterator/#background","title":"Background","text":"<p>In Rust you can create an iterator by implementing 'Iterator' trait which requires you to implement a single method 'next(&amp;mut self)'. We can then use 'map' to transform the elements emitted by the iterator. In the example below, I create an Iterator that emits 10 integers starting from a specified integer. I then map the iterator to a function that doubles every element.</p> Map example<pre><code>pub struct IntIterator {\n    pub start: u32\n}\n\nimpl Iterator for IntIterator {\n    type Item = u32;\n\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        if self.start == 10 {\n            return None\n        }\n        self.start += 1;\n        return Some(self.start)\n    }\n}\n\nfn main(){\n  let it = IntIterator{start: 0};\n  let double: Vec&lt;u32&gt; = it.map(|x| x*2).collect();\n  println!(\"{:?}\", double); // [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n}\n</code></pre>","tags":["rust"]},{"location":"blog/2025/03/26/why-struct-stditermap-doesnt-constraint-i-to-be-an-iterator/#how-map-is-implemented","title":"How map is implemented?","text":"<p>The map function has no logic and simply returns a 'Map' object. </p>map implementation<pre><code>  fn map&lt;B, F&gt;(self, f: F) -&gt; Map&lt;Self, F&gt;\n  where\n      Self: Sized,\n      F: FnMut(Self::Item) -&gt; B,\n  {\n      Map::new(self, f)\n  }\n</code></pre> <p>Lets look into the Map struct. Map has two fields, the iterator and the function. But note that the struct never forces the 'I' to be of type Iterator, neither does it say anything about F. This was not obvious to me when and probably will not be to most of the Rust beginners.</p> Map struct<pre><code>#[derive(Clone)]\npub struct Map&lt;I, F&gt; {\n    pub(crate) iter: I,\n    f: F,\n}\n</code></pre> <p>Not adding any restriction on the fields has many advantages. Say 'Map' was defined as below:-</p> Map struct<pre><code>#[derive(Clone)]\npub struct Map&lt;I: Iterator, F, B&gt; {\n    pub(crate) iter: Iterator,\n    f: FnMut(I::Item) -&gt; B,\n}\n</code></pre>","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-k3s-cluster/","title":"Setting up K3s cluster","text":"","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-k3s-cluster/#setting-up-k3s-cluster","title":"Setting up K3s cluster","text":"<p>Creating this diary entry for future reference, on how I setup the K3s cluster in our cloud lab. This setup might be too specific for my current needs of Tensile Project. Final setup:-</p> <pre><code>graph TD\n  Manager[\"K3s Server&lt;br/&gt;(10.237.23.157)\"]\n  Worker1[\"K3s Agent&lt;br/&gt;node1\"]\n  Worker2[\"K3s Agent&lt;br/&gt;node2\"]\n  Worker3[\"K3s Agent&lt;br/&gt;node3\"]\n  Worker4[\"K3s Agent&lt;br/&gt;node4\"]\n</code></pre> <ol> <li>Start K3s Server on <code>10.237.23.157</code> of the machines:- <code>curl -sfL https://get.k3s.io | sh -c</code></li> <li>Start K3s Agent on rest of the machines:- <code>curl -sfL https://get.k3s.io | K3S_URL=https://10.237.23.157:6443 K3S_TOKEN= sh -</code>. TOKEN can be found in <code>/var/lib/rancher/k3s/server/node-token</code> on the server.</li> <li>See the connected nodes on the server:- <code>sudo kubectl get nodes</code></li> <li>Copy the <code>/etc/rancher/k3s/k3s.yaml</code> to <code>~/.kube/config</code></li> </ol>","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-k3s-cluster/#setting-up-minio-on-k3s-cluster","title":"Setting up Minio on K3s cluster.","text":"<ol> <li>Install helm.</li> <li>Add minio-operator repo to helm: <code>helm repo add minio-operator https://operator.min.io</code></li> <li>Verify <code>helm search repo minio-operator</code></li> <li>Install minio operator. <pre><code>helm install \\\n  --namespace minio-operator \\\n  --create-namespace \\\n  --set replicaCount=4 \\\n  operator minio-operator/operator\n</code></pre></li> <li>Install minio tenant <pre><code>  helm install --namespace tensile_ckpt \\\n  --create-namespace \\\n  --set tenant.name=tensile \\\n  --set tenant.pools.server=4 \\\n  tensile_minio minio-operator/tenant\n</code></pre></li> <li>Verify:- Forward the UI consile port using:- <pre><code>  kubectl port-forward --address 0.0.0.0  svc/tensile-console 9443 -n tensile-ckpt\n</code></pre> Access:- https://0.0.0.0:9443/</li> </ol>","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-k3s-cluster/#setting-up-harbor-on-k3s-cluster","title":"Setting up Harbor on K3s cluster.","text":"<ol> <li>Install helm.</li> <li>Add goharbor repo to helm: <code>helm repo add harbor https://helm.goharbor.io</code></li> <li>Install the chart. <pre><code>  helm install harbor . -n registry \\\n  --set export.type=nodePort \\\n  --set externalURL=http://10.237.x.x:30002 \n</code></pre></li> <li>Access the portal on http://10.237.x.x:30002. Default username password is admin, Harbor123345</li> <li>On the nodes where the registry is to be used:-<ol> <li>Add <code>\"{insecure-registries\": [\"10.237.x.x:30002\"]}</code> in /etc/docker/daemon.json</li> <li><code>sudo systemctl restart docker</code></li> <li><code>docker login http://10.237.23.157:30002</code>. User can be created throguh the portal</li> <li><code>docker tag docker tag hello-world:latest 10.237.x.x:30002/project_name/hello-world:latest</code></li> <li><code>docker push  10.237.x.x:30002/tensile/hello-world:latest</code></li> </ol> </li> </ol>","tags":["rust"]},{"location":"blog/2024/12/13/how-to-measure-latency-in-data-streaming-systems/","title":"How to measure latency in data streaming systems?","text":""},{"location":"blog/2024/12/13/how-to-measure-latency-in-data-streaming-systems/#how-to-measure-latency-in-data-streaming-systems","title":"How to measure latency in data streaming systems?","text":""},{"location":"blog/2024/12/13/how-to-measure-latency-in-data-streaming-systems/#introduction","title":"Introduction","text":"<p>Measuring latency in batch processing is trivial.</p>"},{"location":"blog/2024/12/13/how-to-measure-latency-in-data-streaming-systems/#references","title":"References","text":"<ol> <li>Benchmarking Distributed Stream Data Processing Systems</li> </ol>"},{"location":"blog/2025/06/25/setting-up-open-telemetry-in-rust/","title":"Setting up open telemetry in Rust","text":""},{"location":"blog/2025/06/25/setting-up-open-telemetry-in-rust/#setting-up-open-telemetry-in-rust","title":"Setting up open telemetry in Rust","text":""},{"location":"blog/2025/06/25/setting-up-open-telemetry-in-rust/#objectives","title":"Objectives","text":"<ol> <li>Observe traces of execution of a distributed system.</li> <li>Should be able to turn tracing off at compile time. </li> </ol>"},{"location":"blog/2025/06/25/setting-up-open-telemetry-in-rust/#research","title":"Research","text":"<p>Initially I was confused with dozens of concepts that were introduced to me when I just want log some things. For example, consider the \"basic\" example provided by opentelemetry crate.</p> WTF is going on here?<pre><code>use opentelemetry_appender_tracing::layer;\nuse opentelemetry_sdk::logs::SdkLoggerProvider;\nuse opentelemetry_sdk::Resource;\nuse tracing::error;\nuse tracing_subscriber::{prelude::*, EnvFilter};\n\nfn main() {\n    let exporter = opentelemetry_stdout::LogExporter::default();\n    let provider: SdkLoggerProvider = SdkLoggerProvider::builder()\n        .with_resource(\n            Resource::builder()\n                .with_service_name(\"log-appender-tracing-example\")\n                .build(),\n        )\n        .with_simple_exporter(exporter)\n        .build();\n\n    let filter_otel = EnvFilter::new(\"info\")\n        .add_directive(\"hyper=off\".parse().unwrap())\n        .add_directive(\"tonic=off\".parse().unwrap())\n        .add_directive(\"h2=off\".parse().unwrap())\n        .add_directive(\"reqwest=off\".parse().unwrap());\n    let otel_layer = layer::OpenTelemetryTracingBridge::new(&amp;provider).with_filter(filter_otel);\n\n    let filter_fmt = EnvFilter::new(\"info\").add_directive(\"opentelemetry=debug\".parse().unwrap());\n    let fmt_layer = tracing_subscriber::fmt::layer()\n        .with_thread_names(true)\n        .with_filter(filter_fmt);\n\n    tracing_subscriber::registry()\n        .with(otel_layer)\n        .with(fmt_layer)\n        .init();\n\n    error!(name: \"my-event-name\", target: \"my-system\", event_id = 20, user_name = \"otel\", user_email = \"otel@opentelemetry.io\", message = \"This is an example message\");\n    let _ = provider.shutdown();\n}\n</code></pre> <p>There is a provider, exporter, otel_layer, fmt_layer, and a subscriber, just to log something.</p> <p>I searched for something simpler and found tokio::tracing-opentelemetry. It builds on top of the opentelemetry and hides some of the implementation detail, which is nice. I want to avoid learning about all the nonsense as long as I can. But even this is not straightforward</p>"},{"location":"blog/2024/12/09/reading-list-research-papers/","title":"Reading list (Research Papers)","text":""},{"location":"blog/2024/12/09/reading-list-research-papers/#reading-list-research-papers","title":"Reading list (Research Papers)","text":"<p>List of research papers relevant in my field (data streaming).</p> <ol> <li>Google Dataflow</li> <li>Timely Dataflow</li> <li>A Catalog of Stream Processing Optimizations</li> </ol>"},{"location":"blog/2024/12/08/reading-listtechnical-articles/","title":"Reading list(Technical articles)","text":""},{"location":"blog/2024/12/08/reading-listtechnical-articles/#reading-listtechnical-articles","title":"Reading list(Technical articles)","text":"<p>List of key articles/videos that enhanced my understanding of computer science topics.</p> <ul> <li>Git, first principle</li> <li>Rust async, first principle</li> <li>Rust testing</li> <li>Latency and Throughtput</li> </ul>"},{"location":"blog/2025/04/17/setting-up-docker-swarm/","title":"Setting up Docker Swarm","text":"","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-docker-swarm/#setting-up-docker-swarm","title":"Setting up Docker Swarm","text":"<p>Creating this diary entry for future reference, on how to setup the docker swarm. This setup might be too specific for my current needs of Tensile Project. Setup I want to achieve:-</p> <pre><code>graph TD\n  Manager[\"Manager Node&lt;br/&gt;(Leader)\"]\n  Worker1[\"Worker Node&lt;br/&gt;node1\"]\n  Worker2[\"Worker Node&lt;br/&gt;node2\"]\n  Worker3[\"Worker Node&lt;br/&gt;node3\"]\n  Worker4[\"Worker Node&lt;br/&gt;node4\"]\n  subgraph Docker Swarm Cluster\n      Manager\n      Worker1\n      Worker2\n      Worker3\n      Worker4\n  end\n  Worker1 --&gt; Service1[\"tensile_worker\"]\n  Worker2 --&gt; Service2[\"tensile_worker\"]\n  Worker3 --&gt; Service3[\"tensile_worker\"]\n  Worker4 --&gt; Service4[\"tensile_worker\"]\n\n  Manager -.-&gt; X1[\"tensile_coordinator\"]\n</code></pre>","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-docker-swarm/#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker is installed on all the host machines that is going to be part of the cluster.</li> <li>All host machines are able to connect to each other and Port <code>2377</code>, <code>7946</code> and <code>4789</code> are open.</li> <li>I also made sure all of them have the same version of docker engine. I dont know if this is required or not</li> </ol>","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-docker-swarm/#steps-to-initialize-the-swarm","title":"Steps to initialize the Swarm.","text":"<ol> <li>On the Manager node, initialize the swarm using: <code>docker swarm init --advertise-addr &lt;MANAGER-IP&gt;</code></li> <li>Make the worker nodes join the swarm using <code>docker swarm join --token &lt;TOKEN&gt; &lt;MANAGER_ID&gt;:2377</code>. You will get the token when you initialized the swarm on the manager node.</li> <li>Check if the nodes have joined the cluster using <code>docker node ls</code>. For example it outputs following in my case.</li> </ol> ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION o9hyne818eeig834qe32arhy8 * dell-OptiPlex-3060 Ready Active Leader 28.0.4 xxurgpcq11twzfyrf5epf3yro node1 Ready Active 28.0.4 ggvty0hj0j3f32dbe3rqzw8k5 node2 Ready Active 28.0.4 8dfwhodu6qgt4igbqu8v4uv0a node3 Ready Active 28.0.4 ibc0jo9o849ae1sy1ypld7rk6 node4 Ready Active 28.0.4","tags":["rust"]},{"location":"blog/2025/04/17/setting-up-docker-swarm/#steps-to-start-the-service","title":"Steps to start the service.","text":"<p>Now I want to start a single Tensile Worker service on each node (4 in total). I have already built the image of the Tensile worker.</p> <ol> <li> <p>First label the node1, node2, node3 and node4 with the role of tensile worker. This is required because we dont want to spwan a worker on the manager node. Run this on the manager node. <code>docker node update --label-add role=tensie_worker node1</code></p> </li> <li> <p>Now I start the service using following. Note that we also spawn them in a global mode, because we only want one instance per node. Also we are using a local docker registry hosted on <code>10.237.23.157</code>. <code>--publish mode=host</code> will publish the given port on each node, i.e. you can access port 6000 on each node individually.</p> </li> </ol> <pre><code>docker service create \\\n  --name tensile_worker \\\n  --mode global \\\n  --constraint 'node.labels.role == tensile_worker' \\\n  --restart-condition any \\\n  --env DATABASE_URL=\"blahblah\" \\\n  --publish mode=host,target=6000,published=6000 \\\n  10.237.23.157:5000/tensile-worker:latest\n</code></pre>","tags":["rust"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/system/","title":"system","text":""},{"location":"blog/category/telemetry/","title":"telemetry","text":""},{"location":"blog/category/dev-diary/","title":"dev-diary","text":""},{"location":"blog/category/research-diary/","title":"research-diary","text":""},{"location":"blog/category/paper/","title":"paper","text":""},{"location":"blog/category/streaming/","title":"streaming","text":""},{"location":"blog/category/latency/","title":"latency","text":""},{"location":"blog/category/article/","title":"article","text":""},{"location":"blog/category/list/","title":"list","text":""}]}